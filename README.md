# CEIA-ViT 
---
## Programa de la materia 

1. Arquitectura de Transformers e imágenes como secuencias.
2. Arquitecturas de ViT y el mecanismo de Attention.
3. Ecosistema actual: Hugging Face y modelos preentrenados.
4. GPT en NLP e ImageGPT.
5. Modelos multimodales: combinación de visión y lenguaje (CLIP, DALL-E, ..)
6. Segmentación con SAM y herramientas de auto etiquetado multimodales.
7. OCR y detección con modelos multimodales.
8. Presentación de los proyectos.

---

## **Forma de Evaluación**

### 1. **Entrega de Trabajos Prácticos (Obligatoria - Individual)**

La entrega de los trabajos prácticos es obligatoria y debe ser realizada de manera individual. Los plazos de entrega son los siguientes:

- **Ejercicio 1:** Debe ser entregado antes de la **Clase 3**.
- **Ejercicio 2:** Debe ser entregado antes de la **Clase 4**.
- **Ejercicio 3:** Debe ser entregado antes de la **Clase 5**.
- **Ejercicio 4:** Optativo

---

### 2. **Entrega del Proyecto (Obligatoria - Grupal)**

El proyecto debe ser entregado por grupos e incluir los siguientes elementos:

- **Código funcional:** El código debe estar correctamente implementado y listo para su ejecución.
- **Informe técnico:** Debe contener:
  - Los pasos seguidos en el desarrollo del proyecto.
  - Decisiones de diseño del modelo.
  - Análisis detallado de los resultados.
  - Visualizaciones generadas.
- **Presentación final:** Duración de 15 minutos, enfocada en:
  - Resultados más destacados.
  - Visualizaciones del modelo.
  - Explicación de cómo el modelo puede aplicarse en un contexto real.

El código y el informe deben ser entregados a más tardar el **lunes anterior a la clase 8**. 

---

## **Evaluación del Proyecto**

El proyecto será evaluado de acuerdo a los siguientes criterios:

| **Criterio**                     | **Ponderación** |
|-----------------------------------|-----------------|
| **Claridad conceptual**           | 25%             |
| **Calidad del código**            | 25%             |
| **Evaluación y análisis**         | 25%             |
| **Explicabilidad y visualización** | 25%             |


---

#### **Cálculo de la Evaluación Global**

La evaluación final se calculará mediante la siguiente fórmula:

**Evaluación Global = 0.4 * Prácticas + 0.6 * Proyecto**


## Bibliografía

Rothman, D. (2024) "Transformers for Natural Language Processing and Computer Vision: Explore Generative AI and Large Language Models with Hugging Face, ChatGPT, GPT-4V, and DALL-E." Packt Publishing; 3rd edition.

Dosovitskiy, A., et al. (2020) "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." arXiv preprint arXiv:2010.11929.
[Link](https://arxiv.org/abs/2010.11929)

Vaswani, A., et al. (2017) "Attention is All You Need." Advances in Neural Information Processing Systems (NeurIPS).
[Link](https://arxiv.org/abs/1706.03762)

Haoran Z., et al. (2023) "Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective"
[Link](https://arxiv.org/pdf/2302.03751)

Touvron, H., et al. (2021) "Training data-efficient image transformers & distillation through attention." International Conference on Machine Learning (ICML).
[Link](https://arxiv.org/abs/2012.12877)

Carion, N., et al. (2020) "End-to-End Object Detection with Transformers." European Conference on Computer Vision (ECCV).
[Link](https://arxiv.org/abs/2005.12872)

Wu, B., et al. (2021) "CvT: Introducing Convolutions to Vision Transformers." International Conference on Computer Vision (ICCV).
[Link](https://arxiv.org/abs/2103.15808)

Yuan, L., et al. (2021) "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet." IEEE International Conference on Computer Vision (ICCV).
[Link](https://arxiv.org/abs/2101.11986)



### Docentes a cargo: 

Esp. Abraham Rodriguez (abraham.rodz17@gmail.com); Mg. Oksana Bokhonok (bokhonokok@gmail.com)


